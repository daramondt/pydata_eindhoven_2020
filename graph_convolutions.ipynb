{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with graph convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Dennis Ramondt\n",
    "\n",
    "**Conference**: PyData Eindhoven 2020\n",
    "\n",
    "**Talk**: Monitoring a TV streaming service with AI - from PageRank to graph convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an primer to using graph convolutions, simple node embeddings and supervised pagerank. We use the well known public `Cora` dataset.\n",
    "\n",
    "All you'll need is `tensorflow 2.1.0`, as it's listed in the requirements.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First loading some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'cora'\n",
    "X, A, y = load_data(path='gcn/data/cora/', dataset=DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = A.shape[1]\n",
    "num_features = X.shape[1]\n",
    "num_categories = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple utility function to create training masks. These can be used during training to effectively hold out some part of the training data for validation afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do proper graph deep learning, it's important to normalize the feature and adjacency matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X /= X.sum(1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ = preprocess_adj(A, symmetric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Graph Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic graph convolution, as outlined in the talk, works by propagating node feature information using the adjacency matrix. This implementation is inspired by Thomas Kipf's seminal paper: https://arxiv.org/pdf/1609.02907.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Add, Dropout\n",
    "from gcn.layers.graph import PersonalizedPageRank, GraphConvolution\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the graph convolutions working, we need to pass both the feature matrix and the adjacency matrix as Layer inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Input(shape=(num_nodes, num_nodes))\n",
    "X_in = Input(shape=(num_nodes, num_features,))\n",
    "graph = [np.expand_dims(X, 0), np.expand_dims(A_.todense(), 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers can be stacked at will, depending on how far you want the messages to be passed through the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "H = Dropout(0.5)(H)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "H = Dropout(0.5)(H)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "Y = Dense(num_categories, activation='softmax')(H)\n",
    "\n",
    "model = Model(inputs=[X_in,G], outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 1 samples\n",
      "Epoch 1/50\n",
      "1/1 - 1s - loss: 0.1006\n",
      "Epoch 2/50\n",
      "1/1 - 0s - loss: 0.0998\n",
      "Epoch 3/50\n",
      "1/1 - 0s - loss: 0.0988\n",
      "Epoch 4/50\n",
      "1/1 - 0s - loss: 0.0976\n",
      "Epoch 5/50\n",
      "1/1 - 0s - loss: 0.0959\n",
      "Epoch 6/50\n",
      "1/1 - 0s - loss: 0.0950\n",
      "Epoch 7/50\n",
      "1/1 - 0s - loss: 0.0936\n",
      "Epoch 8/50\n",
      "1/1 - 0s - loss: 0.0935\n",
      "Epoch 9/50\n",
      "1/1 - 0s - loss: 0.0934\n",
      "Epoch 10/50\n",
      "1/1 - 0s - loss: 0.0935\n",
      "Epoch 11/50\n",
      "1/1 - 0s - loss: 0.0922\n",
      "Epoch 12/50\n",
      "1/1 - 0s - loss: 0.0909\n",
      "Epoch 13/50\n",
      "1/1 - 0s - loss: 0.0904\n",
      "Epoch 14/50\n",
      "1/1 - 0s - loss: 0.0895\n",
      "Epoch 15/50\n",
      "1/1 - 0s - loss: 0.0884\n",
      "Epoch 16/50\n",
      "1/1 - 0s - loss: 0.0875\n",
      "Epoch 17/50\n",
      "1/1 - 0s - loss: 0.0871\n",
      "Epoch 18/50\n",
      "1/1 - 0s - loss: 0.0858\n",
      "Epoch 19/50\n",
      "1/1 - 0s - loss: 0.0829\n",
      "Epoch 20/50\n",
      "1/1 - 0s - loss: 0.0812\n",
      "Epoch 21/50\n",
      "1/1 - 0s - loss: 0.0786\n",
      "Epoch 22/50\n",
      "1/1 - 0s - loss: 0.0762\n",
      "Epoch 23/50\n",
      "1/1 - 0s - loss: 0.0747\n",
      "Epoch 24/50\n",
      "1/1 - 0s - loss: 0.0726\n",
      "Epoch 25/50\n",
      "1/1 - 0s - loss: 0.0706\n",
      "Epoch 26/50\n",
      "1/1 - 0s - loss: 0.0678\n",
      "Epoch 27/50\n",
      "1/1 - 0s - loss: 0.0646\n",
      "Epoch 28/50\n",
      "1/1 - 0s - loss: 0.0629\n",
      "Epoch 29/50\n",
      "1/1 - 0s - loss: 0.0605\n",
      "Epoch 30/50\n",
      "1/1 - 0s - loss: 0.0601\n",
      "Epoch 31/50\n",
      "1/1 - 0s - loss: 0.0581\n",
      "Epoch 32/50\n",
      "1/1 - 0s - loss: 0.0558\n",
      "Epoch 33/50\n",
      "1/1 - 0s - loss: 0.0544\n",
      "Epoch 34/50\n",
      "1/1 - 0s - loss: 0.0540\n",
      "Epoch 35/50\n",
      "1/1 - 0s - loss: 0.0514\n",
      "Epoch 36/50\n",
      "1/1 - 0s - loss: 0.0487\n",
      "Epoch 37/50\n",
      "1/1 - 0s - loss: 0.0466\n",
      "Epoch 38/50\n",
      "1/1 - 0s - loss: 0.0444\n",
      "Epoch 39/50\n",
      "1/1 - 0s - loss: 0.0435\n",
      "Epoch 40/50\n",
      "1/1 - 0s - loss: 0.0416\n",
      "Epoch 41/50\n",
      "1/1 - 0s - loss: 0.0386\n",
      "Epoch 42/50\n",
      "1/1 - 0s - loss: 0.0358\n",
      "Epoch 43/50\n",
      "1/1 - 0s - loss: 0.0349\n",
      "Epoch 44/50\n",
      "1/1 - 0s - loss: 0.0319\n",
      "Epoch 45/50\n",
      "1/1 - 0s - loss: 0.0304\n",
      "Epoch 46/50\n",
      "1/1 - 0s - loss: 0.0276\n",
      "Epoch 47/50\n",
      "1/1 - 0s - loss: 0.0261\n",
      "Epoch 48/50\n",
      "1/1 - 0s - loss: 0.0240\n",
      "Epoch 49/50\n",
      "1/1 - 0s - loss: 0.0236\n",
      "Epoch 50/50\n",
      "1/1 - 0s - loss: 0.0252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc46a922e10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(graph, np.expand_dims(y_train, 0), sample_weight=np.expand_dims(train_mask, 0),\n",
    "          batch_size=num_nodes, epochs=50, shuffle=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(graph, batch_size=A.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the predictions, we use the indices for the training mask to split into validation and training sets to see the accuracy and categorical cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loss, train_val_acc = evaluate_preds(preds[0], [y_train, y_val],\n",
    "                                               [idx_train, idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9285714285714286, 0.75]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the validation accuracy is around 75%. Optimal benchmarks for `Cora` are somewhere in the 80s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Personalized PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach is to use the pagerank algorithm to propagate information through the graph. It also uses the adjacency matrix, but can pass messages deeper in the graph due to the multiple (here 10) power iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it doesn't use a kernel to weight the message passing so will not learn which adjacency weights to prefer during propagation. This means the algorithm is not suitable when the adjacency matrix is very sparse and the node features themselves contain little signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, a nice feature is that it is used as a final layer, hence you can use normal dense layers to learn information from the features themselves before propagating the learned representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = Dense(64)(X_in)\n",
    "H = Dense(64)(H)\n",
    "H = Dense(num_categories, activation='softmax')(H)\n",
    "Y = PersonalizedPageRank(alpha=0.1, niter=10, keep_prob=0.5)([H,G])\n",
    "\n",
    "model = Model(inputs=[X_in,G], outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 1 samples\n",
      "Epoch 1/50\n",
      "1/1 - 2s - loss: 0.1007\n",
      "Epoch 2/50\n",
      "1/1 - 0s - loss: 0.0976\n",
      "Epoch 3/50\n",
      "1/1 - 0s - loss: 0.0941\n",
      "Epoch 4/50\n",
      "1/1 - 0s - loss: 0.0910\n",
      "Epoch 5/50\n",
      "1/1 - 0s - loss: 0.0895\n",
      "Epoch 6/50\n",
      "1/1 - 0s - loss: 0.0877\n",
      "Epoch 7/50\n",
      "1/1 - 0s - loss: 0.0848\n",
      "Epoch 8/50\n",
      "1/1 - 0s - loss: 0.0819\n",
      "Epoch 9/50\n",
      "1/1 - 0s - loss: 0.0774\n",
      "Epoch 10/50\n",
      "1/1 - 0s - loss: 0.0738\n",
      "Epoch 11/50\n",
      "1/1 - 0s - loss: 0.0690\n",
      "Epoch 12/50\n",
      "1/1 - 0s - loss: 0.0646\n",
      "Epoch 13/50\n",
      "1/1 - 0s - loss: 0.0592\n",
      "Epoch 14/50\n",
      "1/1 - 0s - loss: 0.0550\n",
      "Epoch 15/50\n",
      "1/1 - 0s - loss: 0.0515\n",
      "Epoch 16/50\n",
      "1/1 - 0s - loss: 0.0454\n",
      "Epoch 17/50\n",
      "1/1 - 0s - loss: 0.0418\n",
      "Epoch 18/50\n",
      "1/1 - 0s - loss: 0.0390\n",
      "Epoch 19/50\n",
      "1/1 - 0s - loss: 0.0367\n",
      "Epoch 20/50\n",
      "1/1 - 0s - loss: 0.0324\n",
      "Epoch 21/50\n",
      "1/1 - 0s - loss: 0.0314\n",
      "Epoch 22/50\n",
      "1/1 - 0s - loss: 0.0282\n",
      "Epoch 23/50\n",
      "1/1 - 0s - loss: 0.0262\n",
      "Epoch 24/50\n",
      "1/1 - 0s - loss: 0.0254\n",
      "Epoch 25/50\n",
      "1/1 - 0s - loss: 0.0234\n",
      "Epoch 26/50\n",
      "1/1 - 0s - loss: 0.0202\n",
      "Epoch 27/50\n",
      "1/1 - 0s - loss: 0.0209\n",
      "Epoch 28/50\n",
      "1/1 - 0s - loss: 0.0219\n",
      "Epoch 29/50\n",
      "1/1 - 0s - loss: 0.0202\n",
      "Epoch 30/50\n",
      "1/1 - 0s - loss: 0.0199\n",
      "Epoch 31/50\n",
      "1/1 - 0s - loss: 0.0185\n",
      "Epoch 32/50\n",
      "1/1 - 0s - loss: 0.0190\n",
      "Epoch 33/50\n",
      "1/1 - 0s - loss: 0.0172\n",
      "Epoch 34/50\n",
      "1/1 - 0s - loss: 0.0182\n",
      "Epoch 35/50\n",
      "1/1 - 0s - loss: 0.0188\n",
      "Epoch 36/50\n",
      "1/1 - 0s - loss: 0.0182\n",
      "Epoch 37/50\n",
      "1/1 - 0s - loss: 0.0171\n",
      "Epoch 38/50\n",
      "1/1 - 0s - loss: 0.0175\n",
      "Epoch 39/50\n",
      "1/1 - 0s - loss: 0.0165\n",
      "Epoch 40/50\n",
      "1/1 - 0s - loss: 0.0172\n",
      "Epoch 41/50\n",
      "1/1 - 0s - loss: 0.0148\n",
      "Epoch 42/50\n",
      "1/1 - 0s - loss: 0.0152\n",
      "Epoch 43/50\n",
      "1/1 - 0s - loss: 0.0170\n",
      "Epoch 44/50\n",
      "1/1 - 0s - loss: 0.0153\n",
      "Epoch 45/50\n",
      "1/1 - 0s - loss: 0.0156\n",
      "Epoch 46/50\n",
      "1/1 - 0s - loss: 0.0159\n",
      "Epoch 47/50\n",
      "1/1 - 0s - loss: 0.0164\n",
      "Epoch 48/50\n",
      "1/1 - 0s - loss: 0.0161\n",
      "Epoch 49/50\n",
      "1/1 - 0s - loss: 0.0167\n",
      "Epoch 50/50\n",
      "1/1 - 0s - loss: 0.0141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc45a4a1d50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(graph, np.expand_dims(y_train, 0), sample_weight=np.expand_dims(train_mask, 0),\n",
    "          batch_size=num_nodes, epochs=50, shuffle=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(graph, batch_size=A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loss, train_val_acc = evaluate_preds(preds[0], [y_train, y_val],\n",
    "                                               [idx_train, idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9714285714285714, 0.83]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we get much higher accuracy here, suggesting that the node features already contain a lot of information and pagerank helps to send this deep enough into the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Node feature embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many real-world scenarios not every node in the graph will be similar. For example, different node types may have different feature sets. This heterogeneity means graph convolutions will not work out of the box. A simple approach to fix this is to first embed the different features into the same dimension and then add them together. After that, simple graph convolutions can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have slices the node feature matrix into two distinct feature sets. We have then given one half of the nodes the first feature set, and the other half the second feature set. Let's see if they are still able to learn anything meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice([0,1], p=[0.5, 0.5], size=num_features)\n",
    "Xs = [X[:,np.where(indices==0)[0]], X[:,np.where(indices==1)[0]]]\n",
    "G = Input(shape=(num_nodes, num_nodes))\n",
    "Xs_in = [Input(shape=(X.shape[0], X.shape[1])) for X in Xs]\n",
    "\n",
    "graphs = [np.expand_dims(X, 0) for X in Xs] + [np.expand_dims(A_.todense(), 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_embed = [Dense(24, use_bias=False)(X) for X in Xs_in]\n",
    "H = Add()(Xs_embed)\n",
    "H = Dropout(0.5)(H)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "H = Dropout(0.5)(H)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "H = Dropout(0.5)(H)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "Y = Dense(num_categories, activation='softmax')(H)\n",
    "\n",
    "model = Model(inputs=[Xs_in,G], outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 1 samples\n",
      "Epoch 1/50\n",
      "1/1 - 1s - loss: 0.1006\n",
      "Epoch 2/50\n",
      "1/1 - 0s - loss: 0.0995\n",
      "Epoch 3/50\n",
      "1/1 - 0s - loss: 0.0981\n",
      "Epoch 4/50\n",
      "1/1 - 0s - loss: 0.0966\n",
      "Epoch 5/50\n",
      "1/1 - 0s - loss: 0.0947\n",
      "Epoch 6/50\n",
      "1/1 - 0s - loss: 0.0934\n",
      "Epoch 7/50\n",
      "1/1 - 0s - loss: 0.0934\n",
      "Epoch 8/50\n",
      "1/1 - 0s - loss: 0.0932\n",
      "Epoch 9/50\n",
      "1/1 - 0s - loss: 0.0923\n",
      "Epoch 10/50\n",
      "1/1 - 0s - loss: 0.0908\n",
      "Epoch 11/50\n",
      "1/1 - 0s - loss: 0.0882\n",
      "Epoch 12/50\n",
      "1/1 - 0s - loss: 0.0870\n",
      "Epoch 13/50\n",
      "1/1 - 0s - loss: 0.0852\n",
      "Epoch 14/50\n",
      "1/1 - 0s - loss: 0.0835\n",
      "Epoch 15/50\n",
      "1/1 - 0s - loss: 0.0813\n",
      "Epoch 16/50\n",
      "1/1 - 0s - loss: 0.0787\n",
      "Epoch 17/50\n",
      "1/1 - 0s - loss: 0.0760\n",
      "Epoch 18/50\n",
      "1/1 - 0s - loss: 0.0748\n",
      "Epoch 19/50\n",
      "1/1 - 0s - loss: 0.0731\n",
      "Epoch 20/50\n",
      "1/1 - 0s - loss: 0.0717\n",
      "Epoch 21/50\n",
      "1/1 - 0s - loss: 0.0691\n",
      "Epoch 22/50\n",
      "1/1 - 0s - loss: 0.0690\n",
      "Epoch 23/50\n",
      "1/1 - 0s - loss: 0.0678\n",
      "Epoch 24/50\n",
      "1/1 - 0s - loss: 0.0667\n",
      "Epoch 25/50\n",
      "1/1 - 0s - loss: 0.0628\n",
      "Epoch 26/50\n",
      "1/1 - 0s - loss: 0.0624\n",
      "Epoch 27/50\n",
      "1/1 - 0s - loss: 0.0587\n",
      "Epoch 28/50\n",
      "1/1 - 0s - loss: 0.0574\n",
      "Epoch 29/50\n",
      "1/1 - 0s - loss: 0.0554\n",
      "Epoch 30/50\n",
      "1/1 - 0s - loss: 0.0512\n",
      "Epoch 31/50\n",
      "1/1 - 0s - loss: 0.0487\n",
      "Epoch 32/50\n",
      "1/1 - 0s - loss: 0.0468\n",
      "Epoch 33/50\n",
      "1/1 - 0s - loss: 0.0451\n",
      "Epoch 34/50\n",
      "1/1 - 0s - loss: 0.0436\n",
      "Epoch 35/50\n",
      "1/1 - 0s - loss: 0.0411\n",
      "Epoch 36/50\n",
      "1/1 - 0s - loss: 0.0381\n",
      "Epoch 37/50\n",
      "1/1 - 0s - loss: 0.0366\n",
      "Epoch 38/50\n",
      "1/1 - 0s - loss: 0.0358\n",
      "Epoch 39/50\n",
      "1/1 - 0s - loss: 0.0330\n",
      "Epoch 40/50\n",
      "1/1 - 0s - loss: 0.0304\n",
      "Epoch 41/50\n",
      "1/1 - 0s - loss: 0.0294\n",
      "Epoch 42/50\n",
      "1/1 - 0s - loss: 0.0279\n",
      "Epoch 43/50\n",
      "1/1 - 0s - loss: 0.0256\n",
      "Epoch 44/50\n",
      "1/1 - 0s - loss: 0.0256\n",
      "Epoch 45/50\n",
      "1/1 - 0s - loss: 0.0218\n",
      "Epoch 46/50\n",
      "1/1 - 0s - loss: 0.0234\n",
      "Epoch 47/50\n",
      "1/1 - 0s - loss: 0.0217\n",
      "Epoch 48/50\n",
      "1/1 - 0s - loss: 0.0180\n",
      "Epoch 49/50\n",
      "1/1 - 0s - loss: 0.0193\n",
      "Epoch 50/50\n",
      "1/1 - 0s - loss: 0.0231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc45aae1e90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(graphs, np.expand_dims(y_train, 0), sample_weight=np.expand_dims(train_mask, 0),\n",
    "          batch_size=num_nodes, epochs=50, shuffle=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(graphs, batch_size=num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loss, train_val_acc = evaluate_preds(preds[0], [y_train, y_val],\n",
    "                                               [idx_train, idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9428571428571428, 0.7033333333333334]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a bit lower than using graph convolutions on the full set of features for all nodes, but that is to be expected given that we have essentially removed half of the meaningful information when simulating the split into two disjoint node types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dramondt_general",
   "language": "python",
   "name": "dramondt_general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
