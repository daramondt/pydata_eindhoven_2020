{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with graph convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Dennis Ramondt\n",
    "\n",
    "**Conference**: PyData Eindhoven 2020\n",
    "\n",
    "**Talk**: Monitoring a TV streaming service with AI - from PageRank to graph convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an primer to using graph convolutions, simple node embeddings and supervised pagerank. We use the well known public `Cora` dataset.\n",
    "\n",
    "All you'll need is `tensorflow 2.1.0`, as it's listed in the requirements.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cora'\n",
    "X, A, y = load_data(path='gcn/data/cora/', dataset=DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = A.shape[1]\n",
    "num_features = X.shape[1]\n",
    "num_categories = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple utility function to create training masks. These can be used during training to effectively hold out some part of the training data for validation afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do proper graph deep learning, it's important to normalize the feature and adjacency matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X /= X.sum(1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ = preprocess_adj(A, symmetric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Graph Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic graph convolution, as outlined in the talk, works by propagating node feature information using the adjacency matrix. This implementation is inspired by Thomas Kipf's seminal paper: https://arxiv.org/pdf/1609.02907.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Add, Dropout\n",
    "from gcn.layers.graph import PersonalizedPageRank, GraphConvolution\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the graph convolutions working, we need to pass both the feature matrix and the adjacency matrix as Layer inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Input(shape=(num_nodes, num_nodes))\n",
    "X_in = Input(shape=(num_nodes, num_features,))\n",
    "graph = [np.expand_dims(X, 0), np.expand_dims(A_.todense(), 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers can be stacked at will, depending on how far you want the messages to be passed through the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "H = Dropout(0.5)(H)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "H = Dropout(0.5)(H)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "Y = Dense(num_categories, activation='softmax')(H)\n",
    "\n",
    "model = Model(inputs=[X_in,G], outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(graph, np.expand_dims(y_train, 0), sample_weight=np.expand_dims(train_mask, 0),\n",
    "          batch_size=num_nodes, epochs=50, shuffle=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(graph, batch_size=A.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the predictions, we use the indices for the training mask to split into validation and training sets to see the accuracy and categorical cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loss, train_val_acc = evaluate_preds(preds[0], [y_train, y_val],\n",
    "                                               [idx_train, idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the validation accuracy is around 75%. Optimal benchmarks for `Cora` are somewhere in the 80s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Personalized PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach is to use the pagerank algorithm to propagate information through the graph. It also uses the adjacency matrix, but can pass messages deeper in the graph due to the multiple (here 10) power iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it doesn't use a kernel to weight the message passing so will not learn which adjacency weights to prefer during propagation. This means the algorithm is not suitable when the adjacency matrix is very sparse and the node features themselves contain little signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, a nice feature is that it is used as a final layer, hence you can use normal dense layers to learn information from the features themselves before propagating the learned representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = Dense(64)(X_in)\n",
    "H = Dense(64)(H)\n",
    "H = Dense(num_categories, activation='softmax')(H)\n",
    "Y = PersonalizedPageRank(alpha=0.1, niter=10, keep_prob=0.5)([H,G])\n",
    "\n",
    "model = Model(inputs=[X_in,G], outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(graph, np.expand_dims(y_train, 0), sample_weight=np.expand_dims(train_mask, 0),\n",
    "          batch_size=num_nodes, epochs=50, shuffle=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(graph, batch_size=A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loss, train_val_acc = evaluate_preds(preds[0], [y_train, y_val],\n",
    "                                               [idx_train, idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we get much higher accuracy here, suggesting that the node features already contain a lot of information and pagerank helps to send this deep enough into the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Node feature embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many real-world scenarios not every node in the graph will be similar. For example, different node types may have different feature sets. This heterogeneity means graph convolutions will not work out of the box. A simple approach to fix this is to first embed the different features into the same dimension and then add them together. After that, simple graph convolutions can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have slices the node feature matrix into two distinct feature sets. We have then given one half of the nodes the first feature set, and the other half the second feature set. Let's see if they are still able to learn anything meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice([0,1], p=[0.5, 0.5], size=num_features)\n",
    "Xs = [X[:,np.where(indices==0)[0]], X[:,np.where(indices==1)[0]]]\n",
    "G = Input(shape=(num_nodes, num_nodes))\n",
    "Xs_in = [Input(shape=(X.shape[0], X.shape[1])) for X in Xs]\n",
    "\n",
    "graphs = [np.expand_dims(X, 0) for X in Xs] + [np.expand_dims(A_.todense(), 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_embed = [Dense(24, use_bias=False)(X) for X in Xs_in]\n",
    "H = Add()(Xs_embed)\n",
    "H = Dropout(0.5)(H)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "H = Dropout(0.5)(H)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "H = Dropout(0.5)(H)\n",
    "H = GraphConvolution(32, activation='relu')([H,G])\n",
    "Y = Dense(num_categories, activation='softmax')(H)\n",
    "\n",
    "model = Model(inputs=[Xs_in,G], outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(graphs, np.expand_dims(y_train, 0), sample_weight=np.expand_dims(train_mask, 0),\n",
    "          batch_size=num_nodes, epochs=50, shuffle=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(graphs, batch_size=num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loss, train_val_acc = evaluate_preds(preds[0], [y_train, y_val],\n",
    "                                               [idx_train, idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a bit lower than using graph convolutions on the full set of features for all nodes, but that is to be expected given that we have essentially removed half of the meaningful information when simulating the split into two disjoint node types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recurrent GCNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Convolutions can also be used in combination with time series. The PEMS-08 traffic dataset consists of 5-minute `flow`, `occupy` and `speed` measurements (aggregated from 30 second raw data) of 170 sensors along a network of highways in the major metropolitan areas in California. The objective is to predict sensor values at some future timestep `T`, making use of the road network structure. In this implementation we choose to predict one of the input features `T` timesteps ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.layers import Reshape, GRU, Lambda, TimeDistributed\n",
    "import tensorflow.keras.backend as K\n",
    "from gcn.layers.graph import GraphConvolution\n",
    "from gcn.utils import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the feature matrix, normalize it, and use the same adjacency matrix normalization as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node features\n",
    "X = np.load('gcn/data/pems/pems08.npz')['data']\n",
    "X /= X.max(axis=0, keepdims=True)\n",
    "X = np.expand_dims(X, 0)\n",
    "\n",
    "# adjacency matrix\n",
    "distance = pd.read_csv('gcn/data/pems/distance.csv').drop_duplicates()\n",
    "A = sp.coo_matrix((distance['cost'], (distance['from'], distance['to'])), shape=(X.shape[2], X.shape[2]))\n",
    "A_ = np.expand_dims(preprocess_adj(A, symmetric=False).todense(), 0).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a sequence length of 1 day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 288\n",
    "n_nodes = A.shape[1]\n",
    "n_features = X.shape[3]\n",
    "train_frac = 0.95\n",
    "train_idx = int(X.shape[1] * train_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_in = Input(shape=(seq_len, n_nodes, n_features,))\n",
    "G = Input(shape=(n_nodes, n_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data generator class returns node feature tensors of length `seq_len` together with the adjacency matrix for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(A_, X[:,:train_idx], target_distance=1, seq_length=seq_len, batch_size=64)\n",
    "test_gen = DataGenerator(A_, X[:,train_idx:], target_distance=1, seq_length=seq_len, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Approach 1: flattening sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and most naieve approach is to flatten the node feature sequences in the first layer into a single feature vector. That way they can be directly be passed through convolutional layers. Although the approach trains fast, there are some methodological concerns as temporal information is essentially jumbled when learning the convolutional weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = Reshape((n_nodes, seq_len * n_features))(X_in)\n",
    "H = GraphConvolution(32 * n_features, activation='relu')([H, G])\n",
    "H = Dropout(0.2)(H)\n",
    "H = GraphConvolution(16 * n_features, activation='relu')([H, G])\n",
    "H = Dropout(0.2)(H)\n",
    "H = Reshape((-1, n_nodes * n_features))(H)\n",
    "H = GRU(32, return_sequences=True)(H)\n",
    "H = Dropout(0.2)(H)\n",
    "H = GRU(64)(H)\n",
    "H = Dropout(0.2)(H)\n",
    "Y = Dense(n_nodes, activation='sigmoid')(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[X_in, G], outputs=Y)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_gen, epochs=5, validation_data=test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the quality of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [t[1] for t in test_gen]\n",
    "y_true = np.concatenate(outputs, axis=0)\n",
    "y_pred = model.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean Squared Error {mean_squared_error(y_true, y_pred)}')\n",
    "print(f'Mean Absolute Error {mean_absolute_error(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(y_true[:,1])\n",
    "plt.plot(y_pred[:,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Approach 2: convolutions at each timestep\n",
    "A more robust approach is to apply a graph convolution at each timestep, before passing the output to a conventional recurrent layer. This approach is inspired by a recent paper from Zhao et al. (2018). They actually build a convolutional layer into the GRU Cell entry gate itself, but the current Keras implementation for recurrent cells is so entangled that it is a chore to replicate and insert the convolutions.\n",
    "\n",
    "Instead, one can apply the graph convolutions in time distributed fashion with Keras, and then pass the result into a conventional recurrent layer. Unfortunately (as is the case with the paper's implementation as well), a full convolution that includes the weight kernel is extremely slow. I therefore chose to only implement the propagation step to reduce the amount of weights that need to be trained.\n",
    "\n",
    "**Reference**\n",
    "Zhao et al. (T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction (IEEE Transactions on Intelligent Transportation Systems 2019))\n",
    "\n",
    "https://arxiv.org/abs/1811.05320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = TimeDistributed(Lambda(lambda x: K.batch_dot(A_, x)))(X_in)\n",
    "H = TimeDistributed(Lambda(lambda x: K.batch_dot(A_, x)))(H)\n",
    "H = Reshape((seq_len, n_nodes * n_features))(H)\n",
    "H = GRU(32, return_sequences=True)(H)\n",
    "H = Dropout(0.2)(H)\n",
    "H = GRU(64)(H)\n",
    "H = Dropout(0.2)(H)\n",
    "Y = Dense(n_nodes, activation='sigmoid')(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[X_in, G], outputs=Y)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_gen, epochs=5, validation_data=test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the quality of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [t[1] for t in test_gen]\n",
    "y_true = np.concatenate(outputs, axis=0)\n",
    "y_pred = model.predict(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean Squared Error {mean_squared_error(y_true, y_pred)}')\n",
    "print(f'Mean Absolute Error {mean_absolute_error(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(y_true[:,1])\n",
    "plt.plot(y_pred[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dramondt_general",
   "language": "python",
   "name": "dramondt_general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
